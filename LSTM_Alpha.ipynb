{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "/*================================================================\n",
    "*   Copyright (C) 2018. All rights reserved.\n",
    "*   Author：Leon Wang\n",
    "*   Date：Sun Mar 26 21:19:35 2017\n",
    "*   Email：leonwang@bu.edu\n",
    "*   Description： Long_Term_Short_Term Memory Neural Network to predict the alpha returns\n",
    "================================================================*/\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rolling_list():\n",
    "    \"\"\"\n",
    "        Object to arrange original data into LSTM input format\n",
    "    \"\"\"\n",
    "    def __init__(self, rolling_window,data,label='NA',pred=[]):\n",
    "        \"\"\"\n",
    "            Method to initialize the original data object\n",
    "            :params rolling_window, interger, number of days in calculating the Rolling Window\n",
    "            :params data, pd.DataFrame, contains n_obs x [Emotions, Returns]\n",
    "        \"\"\"\n",
    "        self.rolling_window=rolling_window\n",
    "        self.data=data\n",
    "        self.length=int(len(data)/rolling_window)\n",
    "        self.label=label\n",
    "        self.pred=pred\n",
    "    def sequence(self):\n",
    "        \"\"\" Method to generate a List of DataFrame for seperated by each date.\n",
    "            :return, List of pd.DataFrame, with List length= self.length and DataFrame length of self.rolling_window\n",
    "        \"\"\"\n",
    "        List=[]\n",
    "        for i in range(len(self.data)-rolling_window):\n",
    "            List.append(self.data[i:i+self.rolling_window])\n",
    "        return List\n",
    "    def recover_alpha(self):\n",
    "        \"\"\"\n",
    "            Method to generate a List of DataFrame seperated by each date.\n",
    "            :return\n",
    "        \"\"\"\n",
    "        List=[]\n",
    "        for i in range(len(self.data)-rolling_window):\n",
    "            without_today=self.data[self.label][i:i+self.rolling_window-1]\n",
    "            List.append((sum(without_today)+self.pred[i])/self.rolling_window)\n",
    "        return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_d_dist(x,y):\n",
    "    value=0\n",
    "    for i in range(len(x)):\n",
    "        value+=(x[i]-y[i])**2\n",
    "    return value\n",
    "distance=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_dict(indi_stock_dir,index_stock_dir,rolling_window=15):\n",
    "    \"\"\"\n",
    "        Function to return a dictionary of stocks with tickers to be the key\n",
    "        : params indi_stock_dir, individual stock directory, contains mutiple files\n",
    "        : params index_stock_dir, index stock directory, ending with csv\n",
    "        : return a dictionary of stocks with:\n",
    "                    * tickers to be the key \n",
    "                    * A 2d-list sorted by date. The List contains individual stock return and index return.\n",
    "        : Example: {'JPM':[['2014-09-15',-0.1584,0.0050],['2014-09-16',-0.1692,0.0075]...etc]}\n",
    "    \"\"\"\n",
    "    # Load the file name \n",
    "    for dira,path,files in os.walk(indi_stock_dir):\n",
    "        1+1\n",
    "    # Regular Expression to extract stock tickers\n",
    "    regex='financial_data_(.*?)\\.csv'\n",
    "    pat=re.compile(regex,re.S)\n",
    "    name=set(pat.findall(str(files)))\n",
    "    \n",
    "    # Read the index_stock data, store it as x for regression\n",
    "    x=pd.read_csv(index_stock_dir,index_col=0)['CLOSE']\n",
    "    x=x.diff(1)\n",
    "    x=sm.add_constant(x)\n",
    "    \n",
    "    \n",
    "    stock_dict={}\n",
    "    stock_dict_w_o_t={}\n",
    "    \n",
    "    for stocks in name:\n",
    "        temp_List=[]\n",
    "        temp_List2=[]\n",
    "        y=pd.read_csv('Twitter_sentiment_DJIA30/financial_data_'+str(stocks)+'.csv',index_col=0)['CLOSE']\n",
    "        y=y.dropna(how=\"any\")\n",
    "        temp=pd.concat([x,y.diff(1)],axis=1).dropna(how=\"any\")\n",
    "        temp.columns=['const','Close_x','Close_y']\n",
    "        observ_num=len(temp)\n",
    "        for i in range(observ_num-rolling_window):\n",
    "            # First Modeing fitting\n",
    "            model = sm.OLS(temp['Close_y'].iloc[i:i+rolling_window],temp[['const','Close_x']].iloc[i:i+rolling_window]).fit()\n",
    "            \n",
    "            # Store the data in the order of [Date, Alpha, Beta]\n",
    "            temp_List.append([temp.index[rolling_window+i],model.params['const'],model.params['Close_x']])\n",
    "            \n",
    "            # Second Modeling fitting\n",
    "            model = sm.OLS(temp['Close_y'].iloc[i:i+rolling_window-1],temp[['const','Close_x']].iloc[i:i+rolling_window-1]).fit()\n",
    "\n",
    "            # Store the data in the order of [Date, Alpha, Beta]\n",
    "            temp_List2.append([temp.index[rolling_window+i],model.params['const'],model.params['Close_x']])\n",
    "        stock_dict.update({stocks:temp_List})\n",
    "        stock_dict_w_o_t.update({stocks:temp_List2})\n",
    "    return stock_dict, stock_dict_w_o_t\n",
    "\n",
    "def agg_tables(stock_dict):\n",
    "    \"\"\"\n",
    "        Function to return alpha_table,beta_table,emotion_table\n",
    "        : param stock_dict, a dictionary of stocks with:\n",
    "                    * tickers to be the key \n",
    "                    * A 2d-list sorted by date. The List contains individual stock return and index return.\n",
    "          Example: {'JPM':[['2014-09-15',-0.1584,0.0050],['2014-09-16',-0.1692,0.0075]...etc]}\n",
    "\n",
    "        : return alpha_table, pd.DataFrame, indexed with datadate, columned with stock tickers, content with the Alpha in regression model\n",
    "                 beta_table, pd.DataFrame, indexed with datadate, columned with stock tickers, content with the Beta in regression model\n",
    "                 emotion_table, pd.DataFrame,indexed with datadate, columned with stock tickers, content with the Emotions in regression model\n",
    "    \"\"\"\n",
    "    ticker_names=list(stock_dict.keys())\n",
    "    ticker_names.remove('DJIA')\n",
    "    init_date=[every_obs[0] for every_obs in stock_dict[ticker_names[0]]]\n",
    "    \n",
    "    emotion_table=pd.DataFrame(index=init_date,columns=ticker_names)\n",
    "    alpha_table=pd.DataFrame(index=init_date,columns=ticker_names)\n",
    "    beta_table=pd.DataFrame(index=init_date,columns=ticker_names)\n",
    "    \n",
    "    for ticker in ticker_names:\n",
    "        date=[every_obs[0] for every_obs in stock_dict[ticker]]\n",
    "        alpha=[alpha[1] for alpha in stock_dict[ticker]]\n",
    "        beta=[beta[2] for beta in stock_dict[ticker]]\n",
    "        \n",
    "        tw_emotion=pd.read_csv('Twitter_sentiment_DJIA30/twitter_data_'+str(ticker)+'.csv',index_col=0)\n",
    "        tw_emotion=tw_emotion[['NUM_NEG','NUM_POS','NUM_NEU']]\n",
    "        profession_emotion=pd.read_csv('Rating/'+str(ticker.lower())+'.csv',index_col=0)['Rating']\n",
    "        profession_emotion.index=pd.to_datetime(profession_emotion.index)\n",
    "        profession_emotion.dropna(how='any')\n",
    "        for m in range(len(date)):\n",
    "            alpha_table[ticker].loc[date[m]]=alpha[m]\n",
    "            beta_table[ticker].loc[date[m]]=beta[m]\n",
    "            emotion_table[ticker].loc[date[m]]=np.hstack((tw_emotion.loc[date[m]].values,profession_emotion.loc[date[m]].values))\n",
    "\n",
    "    return alpha_table,beta_table,emotion_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_stock_dir='Twitter_sentiment_DJIA30'\n",
    "index_stock_dir='financial_data_DJIA.csv'\n",
    "stock_dict,stock_dict_w_o_t=get_stock_dict(indi_stock_dir,index_stock_dir,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,emotion_table=agg_tables(stock_dict)\n",
    "a_w_o_t,b_w_o_t,emotion_table_w_o_t=agg_tables(stock_dict_w_o_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3277409e8700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0msep_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0manalysis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AXP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AXP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"any\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Emotion'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mindexs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrolling_window\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msep_point\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Setting Up Parameters #1\n",
    "rolling_window=15\n",
    "biases=0.1\n",
    "n_inputs=4\n",
    "n_hidden_units=50\n",
    "class_num=1\n",
    "nsteps=rolling_window\n",
    "lr=0.01\n",
    "\n",
    "### Setting Up Parameters #2\n",
    "sess = tf.InteractiveSession()\n",
    "weight={'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units],0,n_inputs**(-1/2))),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_units, class_num],0,n_hidden_units**(-1/2)))\n",
    "                    }\n",
    "biases = {'in': tf.Variable(tf.constant(biases, shape=[n_hidden_units, ])),\n",
    "          'out': tf.Variable(tf.constant(biases, shape=[class_num, ]))\n",
    "                    }\n",
    "x=tf.placeholder(tf.float32, [nsteps, n_inputs])\n",
    "y=tf.placeholder(tf.float32, [class_num])\n",
    "\n",
    "\n",
    "\n",
    "### Setting Up Training\n",
    "x_in=tf.matmul(x,weight['in'])+biases['in']\n",
    "x_in=tf.reshape(x_in,[1,nsteps,n_hidden_units])\n",
    "\n",
    "cell=tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "init_state = cell.zero_state(1, dtype=tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, x_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "predict = tf.matmul(final_state[1], weight['out']) + biases['out']\n",
    "squared_deltas = tf.square(predict - y)\n",
    "\n",
    "cost = tf.reduce_mean(squared_deltas)\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "sep_point=150\n",
    "analysis=pd.concat([emotion_table['AXP'],table['AXP']],axis=1).dropna(how=\"any\")\n",
    "analysis.columns=['Emotion','Return']\n",
    "indexs=analysis.index[rolling_window+sep_point:]\n",
    "agg_table=pd.DataFrame(index=indexs)\n",
    "\n",
    "for m in name:\n",
    "    analysis=pd.concat([emotion_table[m],a[m]],axis=1).dropna(how=\"any\")\n",
    "    analysis.columns=['Emotion','Return']\n",
    "    pred_alpha=[]\n",
    "    pred_beta=[]\n",
    "    real_alpha=[]\n",
    "    real_beta=[]\n",
    "    sess.run(init)\n",
    "    for i in rolling_list(rolling_window,analysis).sequence():\n",
    "        x_batch=[list(w) for w in i['Emotion'].values]\n",
    "        x_batch=np.array(x_batch)\n",
    "        y_batch=np.array(i[['Return']].values[-1]).reshape(class_num,)\n",
    "        sess.run(train,feed_dict={x:x_batch,y:y_batch})\n",
    "        \n",
    "        pred_alpha.append(sess.run(predict,feed_dict={x:x_batch,y:y_batch})[0][0])\n",
    "        real_alpha.append(y_batch[0])\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(range(len(real_alpha[sep_point:])),real_alpha[sep_point:],label='Real Alpha')\n",
    "    plt.plot(range(len(pred_alpha[sep_point:])),pred_alpha[sep_point:],label='Predicted Alpha (LSTM)')\n",
    "    plt.ylim((-0.2, 0.2))\n",
    "    plt.legend()   \n",
    "    plt.title('Prediction for MA '+str(rolling_window)+' Days, Symbol='+str(m)+'')\n",
    "    plt.show()\n",
    "    ### Recover the alpha\n",
    "#    agg_table[m]=rolling_list(rolling_window,analysis,'Return',prediction).recover_alpha()[sep_point:]\n",
    "    \n",
    "    agg_table[m]=pred_alpha[sep_point:]\n",
    "    distance.append(e_d_dist(real_alpha[sep_point:],pred_alpha[sep_point:]))\n",
    "\"\"\"\n",
    "agg_table.to_csv(\"agg_table.csv\")\n",
    "decision=agg_table-table_m_o\n",
    "decision=decision.drop('DJIA',axis=1)\n",
    "decision=decision.dropna(how='any')\n",
    "decision=decision>0.1\n",
    "decision=decision[:-1]\n",
    "\n",
    "### Begin 22\n",
    "real_return=pd.DataFrame()\n",
    "name=set(pat.findall(str(files)))\n",
    "for stocks in name:\n",
    "    y=pd.read_csv('Twitter_sentiment_DJIA30/financial_data_'+str(stocks)+'.csv',index_col=0)['CLOSE']\n",
    "    y=y.diff(1)[1:]/y[:-1].values\n",
    "    y=y.dropna(how=\"any\")\n",
    "    real_return[stocks]=y\n",
    "real_return=real_return.dropna(how='any')\n",
    "index_return=real_return['DJIA']\n",
    "real_return=real_return.drop('DJIA',axis=1)\n",
    "beta_table=beta_table.sort_index(axis=1)\n",
    "beta_table=beta_table.drop('DJIA',axis=1)\n",
    "real_return=real_return.sort_index(axis=1)\n",
    "\n",
    "indexs=list(name)\n",
    "indexs.remove(\"DJIA\")\n",
    "distance=pd.DataFrame(distance,index=indexs)\n",
    "distance=distance.sort_index(axis=1)\n",
    "bad_prediction=distance[distance>3].dropna(how='any').index\n",
    "distance.plot()\n",
    "plt.show()\n",
    "return_continous_port=[]\n",
    "return_continous_index=[]\n",
    "cmre=1\n",
    "cmre_index=1\n",
    "### drop bad prediction\n",
    "decision=decision.drop(bad_prediction,axis=1)\n",
    "real_return=real_return.drop(bad_prediction,axis=1)\n",
    "beta_table=beta_table.drop(bad_prediction,axis=1)\n",
    "### end of dropping\n",
    "for i in decision.index:\n",
    "    boolen=decision.loc[i]==True\n",
    "    ###\n",
    "    long=np.mean(real_return.loc[i][boolen]+beta_table.loc[i][boolen]*index_return.loc[i])\n",
    "    #beta_table.loc[i][boolen]\n",
    "    boolen=boolen==False\n",
    "    short=np.mean(real_return.loc[i][boolen]+beta_table.loc[i][boolen]*index_return.loc[i])\n",
    "#    cmre*=1+(long-short)\n",
    "    cmre*=1+(long-short)\n",
    "    return_continous_port.append(cmre)\n",
    "    \n",
    "    cmre_index*=1+index_return.loc[i]\n",
    "    return_continous_index.append(cmre_index)\n",
    "\n",
    "\n",
    "plt.plot(return_continous_index,label=\"Index\")\n",
    "plt.plot(return_continous_port,label=\"Portfolio\")\n",
    "plt.legend()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
